{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1eb3ba98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import ClassifierI\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from statistics import mode\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "import re\n",
    "from TurkishStemmer import TurkishStemmer\n",
    "from zeyrek import MorphAnalyzer\n",
    "import logging\n",
    "from zemberek import TurkishMorphology, TurkishSentenceNormalizer, TurkishTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "50d4036e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "df = pd.read_excel('formasyon_veri_yanitlari.xlsx')\n",
    "df = df.drop(df.columns[[0, 1]], axis=1)\n",
    "\n",
    "# Extract text-label pairs\n",
    "text_label_list = []\n",
    "for column in df.columns:\n",
    "    label = column.split('-')[0]\n",
    "    for sentence in df[column].dropna():\n",
    "        text_label_list.append((sentence, label))\n",
    "\n",
    "text_label_df = pd.DataFrame(text_label_list, columns=['Text', 'Label'])\n",
    "# shuffle your dataframe in-place and reset the index\n",
    "text_label_df = text_label_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "cd34e516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-02 20:35:34,932 - zemberek.morphology.turkish_morphology - INFO\n",
      "Msg: TurkishMorphology instance initialized in 12.686001539230347\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Zemberek tools\n",
    "tokenizer = TurkishTokenizer.DEFAULT\n",
    "stop_words = set(stopwords.words(\"turkish\"))\n",
    "stemmer = TurkishStemmer()\n",
    "morphology = TurkishMorphology.create_with_defaults()\n",
    "normalizer = TurkishSentenceNormalizer(morphology)\n",
    "\n",
    "# Text Preprocessing\n",
    "def preprocess_text(text):\n",
    "    # ^ Matches the start of the string\n",
    "    # If the first character of the set is '^', all the characters that are not in the set will be matched\n",
    "    # \\ signals a special sequence\n",
    "    # [] Used to indicate a set of characters\n",
    "    # Remove non-alphanumeric\n",
    "    # remove any new leading and trailing whitespace\n",
    "    text = re.sub(r\"[^\\sa-zA-Z0-9ğüşöçıĞÜŞİÖÇ]\", \"\", text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    # normalize\n",
    "    # attempt to reduce its randomness, bringing it closer to a predefined “standard”\n",
    "    text = normalizer.normalize(text.lower())\n",
    "    #print(text)\n",
    "    \n",
    "    # tokenize\n",
    "    # it’s the task of cutting a text into pieces called tokens.\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    words = [\n",
    "        stemmer.stem(token.content)\n",
    "        for token in tokens\n",
    "        if token.content not in stop_words\n",
    "    ]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply preprocessing\n",
    "text_label_df['Processed_Text'] = text_label_df['Text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "817c11fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common 15 words:\n",
      "formasyon: 133\n",
      "v: 128\n",
      "voleybol: 102\n",
      "üçgen: 91\n",
      "ok: 78\n",
      "oluş: 77\n",
      "şekl: 77\n",
      "çizg: 73\n",
      "baş: 62\n",
      "bir: 53\n",
      "yap: 49\n",
      "drönelar: 49\n",
      "çiz: 46\n",
      "ters: 29\n",
      "ol: 28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "for text in text_label_df['Processed_Text']:\n",
    "    all_words.extend(text.split())\n",
    "\n",
    "all_words_dist = nltk.FreqDist(all_words)\n",
    "most_common_words = '\\n'.join([f\"{word}: {freq}\" for word, freq in all_words_dist.most_common(15)])\n",
    "print(f\"Most common 15 words:\\n{most_common_words}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ea387155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find features\n",
    "word_features = list(all_words_dist.keys())[:400]\n",
    "\n",
    "def find_features(document):\n",
    "    words = set(document.split())\n",
    "    return {word: (word in words) for word in word_features}\n",
    "\n",
    "# Prepare datasets\n",
    "featuresets = [\n",
    "    (find_features(row['Processed_Text']), row['Label'])\n",
    "    for _, row in text_label_df.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "cbc435ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: %92.98245614035088\n",
      "MultinomialNB Accuracy: %92.98245614035088\n",
      "BernoulliNB Accuracy: %93.85964912280701\n",
      "SGDClassifier Accuracy: %95.6140350877193\n",
      "LinearSVC Accuracy: %97.36842105263158\n",
      "LogisticRegression Accuracy: %96.49122807017544\n"
     ]
    }
   ],
   "source": [
    "# datasets\n",
    "train_size = int(len(featuresets) * 0.8)\n",
    "training_set = featuresets[:train_size]\n",
    "testing_set = featuresets[train_size:]\n",
    "\n",
    "# Classifiers\n",
    "classifiers = {\n",
    "    \"Naive Bayes\": nltk.NaiveBayesClassifier.train(training_set),\n",
    "    \"MultinomialNB\": SklearnClassifier(MultinomialNB()).train(training_set),\n",
    "    \"BernoulliNB\": SklearnClassifier(BernoulliNB()).train(training_set),\n",
    "    \"SGDClassifier\": SklearnClassifier(SGDClassifier()).train(training_set),\n",
    "    \"LinearSVC\": SklearnClassifier(LinearSVC(dual=True)).train(training_set),\n",
    "    \"LogisticRegression\": SklearnClassifier(LogisticRegression()).train(training_set),\n",
    "}\n",
    "\n",
    "\n",
    "# Evaluate classifiers\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"{name} Accuracy: %{nltk.classify.accuracy(clf, testing_set) * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b2073543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voted Classifier Accuracy: 95.6140350877193%\n",
      "Classification: voleybol\n",
      "Confidence %: 100.0\n"
     ]
    }
   ],
   "source": [
    "# Voting Classifier\n",
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "\n",
    "    def classify(self, features):\n",
    "        votes = [clf.classify(features) for clf in self._classifiers]\n",
    "        return mode(votes)\n",
    "\n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for classification in self._classifiers:\n",
    "            vote = classification.classify(features)\n",
    "            votes.append(vote)\n",
    "\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf\n",
    "\n",
    "voted_classifier = VoteClassifier(*classifiers.values())\n",
    "print(f\"Voted Classifier Accuracy: {nltk.classify.accuracy(voted_classifier, testing_set) * 100}%\")\n",
    "\n",
    "ex_data = \"zavallı\"\n",
    "\n",
    "print(f\"Classification: {voted_classifier.classify(find_features(ex_data))}\")\n",
    "print(f\"Confidence %: {voted_classifier.confidence(find_features(ex_data))*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "be9f73e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V', 'şek', 'uç', 'üçgen', 'formasyon', 'geçmek', 'Hadi', 'boleybol', 'oynamak', 'Çocuk', 'üçgen', 'formasyon', 'oluşmak', 'gerek', '.', 'voleybol', 'maç', 'yapmak', 'bir', 'nokta', 'bir', 'nokta', 'dümdüz', 'bir', 'çizgi', 'Çek', ',', '...', 'üçgen', 'formasyon', 'oluştururmusun', 'Dronların', 'çizgi', 'üzerinde', 'gitmek', 'istemek']\n"
     ]
    }
   ],
   "source": [
    "# Trying lemmatize\n",
    "\n",
    "from zeyrek import MorphAnalyzer\n",
    "import nltk\n",
    "import logging\n",
    "\n",
    "def run_examples():\n",
    "    logging.getLogger(\"zeyrek.rulebasedanalyzer\").setLevel(logging.ERROR)\n",
    "    analyzer = MorphAnalyzer()\n",
    "    \n",
    "    with open('text.txt', encoding='utf-8') as text_file:\n",
    "        text = text_file.read()\n",
    "\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    words = []\n",
    "    for token in tokens:\n",
    "        content = token.content\n",
    "        if content not in stop_words:\n",
    "            lemmas = analyzer.lemmatize(content)[0][1]\n",
    "            if lemmas:  # Check if there are any lemmas\n",
    "                words.append(lemmas[0])  # Append the first lemma\n",
    "            else:\n",
    "                words.append(content)  # Fallback to the original word\n",
    "    print(words)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_examples()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
