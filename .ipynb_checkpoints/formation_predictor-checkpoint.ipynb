{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9d0f5ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import pickle\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import LinearSVC, NuSVC\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from TurkishStemmer import TurkishStemmer\n",
    "\n",
    "from zemberek import (\n",
    "    TurkishSpellChecker,\n",
    "    TurkishSentenceNormalizer,\n",
    "    TurkishSentenceExtractor,\n",
    "    TurkishMorphology,\n",
    "    TurkishTokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a04854bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>voleybol oyna</td>\n",
       "      <td>voleybol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Voleybol yapsana</td>\n",
       "      <td>voleybol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>çizgi şeklini meydana getir</td>\n",
       "      <td>Çizgi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Voleybol oynar gibi hareket et</td>\n",
       "      <td>voleybol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sürü gösterisine başla</td>\n",
       "      <td>voleybol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>Voleybol oyna</td>\n",
       "      <td>voleybol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>Ok basi gibi v olustur</td>\n",
       "      <td>Ok başı</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>Dronelar aynı doğru üzerine sıralansınlar</td>\n",
       "      <td>Çizgi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>V çiz</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>ilk adimi servis olan spor dalının formasyonun...</td>\n",
       "      <td>voleybol</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>567 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text     Label\n",
       "0                                        voleybol oyna  voleybol\n",
       "1                                     Voleybol yapsana  voleybol\n",
       "2                          çizgi şeklini meydana getir     Çizgi\n",
       "3                       Voleybol oynar gibi hareket et  voleybol\n",
       "4                               sürü gösterisine başla  voleybol\n",
       "..                                                 ...       ...\n",
       "562                                      Voleybol oyna  voleybol\n",
       "563                             Ok basi gibi v olustur   Ok başı\n",
       "564          Dronelar aynı doğru üzerine sıralansınlar     Çizgi\n",
       "565                                              V çiz         V\n",
       "566  ilk adimi servis olan spor dalının formasyonun...  voleybol\n",
       "\n",
       "[567 rows x 2 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('formasyon_veri_yanitlari.xlsx')\n",
    "df = df.drop(df.columns[[0,1]], axis=1)\n",
    "\n",
    "text_label_list = []\n",
    "sentences = []\n",
    "for column in df.columns:\n",
    "    label = column.split('-')[0]\n",
    "    \n",
    "    for sentence in df[column]:\n",
    "        sentences.append(sentence)\n",
    "        text_label_list.append((sentence, label))   \n",
    "\n",
    "text_label_df = pd.DataFrame(text_label_list, columns =['Text', 'Label'])\n",
    "text_label_df = text_label_df.sample(frac=1).reset_index(drop=True)\n",
    "text_label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3920c178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-02 17:49:37,236 - zemberek.morphology.turkish_morphology - INFO\n",
      "Msg: TurkishMorphology instance initialized in 23.174827575683594\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(sentences)\n",
    "\n",
    "# Morphology also analyzes how words behave as parts of speech, \n",
    "# and how they may be inflected to express grammatical categories including number, tense, and aspect\n",
    "morphology = TurkishMorphology.create_with_defaults() \n",
    "all_words = []\n",
    "\n",
    "# attempt to reduce its randomness, bringing it closer to a predefined “standard”\n",
    "normalizer = TurkishSentenceNormalizer(morphology) \n",
    "\n",
    "# it’s the task of cutting a text into pieces called tokens.\n",
    "tokenizer = TurkishTokenizer.DEFAULT\n",
    "\n",
    "# Stemming is the process of reducing the words to their word stem or root form\n",
    "stemmer = TurkishStemmer()\n",
    "stop_words = set(stopwords.words(\"turkish\"))\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence = normalizer.normalize(sentence)\n",
    "    \n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    words = [\n",
    "        stemmer.stem(token.content)\n",
    "        for token in tokens\n",
    "        if token.content not in stop_words\n",
    "    ]\n",
    "    \n",
    "    for word in words:\n",
    "        if word.isalpha():\n",
    "            all_words.append(word.lower())\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "26fa23ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common 15 words:\n",
      "formasyon: 133\n",
      "v: 130\n",
      "voleybol: 102\n",
      "üçgen: 91\n",
      "ok: 78\n",
      "oluş: 77\n",
      "şekl: 77\n",
      "çizg: 73\n",
      "baş: 62\n",
      "bir: 53\n",
      "yap: 49\n",
      "drönelar: 49\n",
      "çiz: 46\n",
      "ters: 29\n",
      "ol: 28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_words_dist = nltk.FreqDist(all_words)\n",
    "most_common_words = '\\n'.join([f\"{word}: {freq}\" for word, freq in all_words_dist.most_common(15)])\n",
    "print(f\"Most common 15 words:\\n{most_common_words}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "869313f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = list(all_words_dist.keys())[:300]\n",
    "\n",
    "def find_features(document):\n",
    "    words = set(nltk.word_tokenize(document.lower()))\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features\n",
    "\n",
    "\n",
    "feature_sets = [(find_features(text), label) for (text, label) in text_label_list]\n",
    "\n",
    "train_size = int(0.8 * len(feature_sets))  # 80% \n",
    "training_set = feature_sets[:train_size]\n",
    "testing_set = feature_sets[train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f8533c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Algorithm accuracy percent: 75.43859649122807\n",
      "Most Informative Features\n",
      "                      ok = True           Ok baş : V      =     48.3 : 1.0\n",
      "                   üçgen = True            Üçgen : V      =     32.6 : 1.0\n",
      "                       v = False           Çizgi : V      =     14.5 : 1.0\n",
      "                   şekil = True            Üçgen : Ok baş =      9.0 : 1.0\n",
      "                     geç = True           voleyb : Üçgen  =      8.3 : 1.0\n",
      "                     bir = True            Çizgi : V      =      8.2 : 1.0\n",
      "                    olan = True            Üçgen : Çizgi  =      5.0 : 1.0\n",
      "                     yap = True           voleyb : Çizgi  =      5.0 : 1.0\n",
      "                      et = True           voleyb : Çizgi  =      5.0 : 1.0\n",
      "                   geçiş = True           voleyb : Ok baş =      5.0 : 1.0\n",
      "              kullanarak = True           voleyb : Üçgen  =      5.0 : 1.0\n",
      "                     var = True           voleyb : Üçgen  =      5.0 : 1.0\n",
      "                voleybol = False          Ok baş : voleyb =      4.9 : 1.0\n",
      "                     hat = True            Çizgi : V      =      4.6 : 1.0\n",
      "                   üçgen = False          Ok baş : Üçgen  =      3.9 : 1.0\n"
     ]
    }
   ],
   "source": [
    "NBclassifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "print(\"Naive Bayes Algorithm accuracy percent:\", (nltk.classify.accuracy(NBclassifier, testing_set))*100)\n",
    "NBclassifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5e815be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB Classifier accuracy percent: 71.05263157894737\n"
     ]
    }
   ],
   "source": [
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"MNB Classifier accuracy percent:\", (nltk.classify.accuracy(MNB_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "82dd8bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB Classifier accuracy percent: 0.0\n"
     ]
    }
   ],
   "source": [
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(training_set)\n",
    "print(\"BernoulliNB Classifier accuracy percent:\", (nltk.classify.accuracy(BernoulliNB_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3911dbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier_classifier accuracy percent: 74.56140350877193\n"
     ]
    }
   ],
   "source": [
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(training_set)\n",
    "print(\"SGDClassifier_classifier accuracy percent:\", (nltk.classify.accuracy(SGDClassifier_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6dd3783d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression_classifier accuracy percent: 69.2982456140351\n"
     ]
    }
   ],
   "source": [
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d58b8c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC_classifier accuracy percent: 69.2982456140351\n"
     ]
    }
   ],
   "source": [
    "LinearSVC_classifier = SklearnClassifier(LinearSVC(dual=True))\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(\"LinearSVC_classifier accuracy percent:\", (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "37e3afa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "\n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for classification in self._classifiers:\n",
    "            vote = classification.classify(features)\n",
    "            votes.append(vote)\n",
    "        return mode(votes)\n",
    "\n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for classification in self._classifiers:\n",
    "            vote = classification.classify(features)\n",
    "            votes.append(vote)\n",
    "\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "beb8efc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voted_classifier accuracy percent: 72.80701754385966\n",
      "Classification: Çizgi\n",
      "Confidence %: 100.0\n"
     ]
    }
   ],
   "source": [
    "voted_classifier = VoteClassifier(NBclassifier,MNB_classifier,BernoulliNB_classifier,LogisticRegression_classifier,SGDClassifier_classifier,LinearSVC_classifier)\n",
    "print(\"Voted_classifier accuracy percent:\", (nltk.classify.accuracy(voted_classifier, testing_set))*100)\n",
    "\n",
    "ex_data = \"drone'lar bugün çizgi olacaksınız\"\n",
    "\n",
    "print(f\"Classification: {voted_classifier.classify(find_features(ex_data))}\")\n",
    "print(f\"Confidence %: {voted_classifier.confidence(find_features(ex_data))*100}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dd2799b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-02 17:51:33,875 - zemberek.morphology.turkish_morphology - INFO\n",
      "Msg: TurkishMorphology instance initialized in 17.299692392349243\n",
      "\n",
      "2025-01-02 17:51:54,309 - __main__ - INFO\n",
      "Msg: Normalization instance created in: 20.433939456939697 s\n",
      "\n",
      "Yrn okua gidicem\n",
      "yarın okula gideceğim \n",
      "\n",
      "Tmm, yarin havuza giricem ve aksama kadar yaticam :)\n",
      "tamam , yarın havuza gireceğim ve akşama kadar yatacağım :) \n",
      "\n",
      "ah aynen ya annemde fark ettı siz evinizden cıkmayın diyo\n",
      "ah aynen ya annemde fark etti siz evinizden çıkmayın diyor \n",
      "\n",
      "gercek mı bu? Yuh! Artık unutulması bile beklenmiyo\n",
      "gerçek mi bu ? yuh ! artık unutulması bile beklenmiyor \n",
      "\n",
      "Hayır hayat telaşm olmasa alacam buraları gökdelen dikicem.\n",
      "hayır hayat telaşı olmasa alacağım buraları gökdelen dikeceğim . \n",
      "\n",
      "yok hocam kesınlıkle oyle birşey yok\n",
      "yok hocam kesinlikle öyle bir şey yok \n",
      "\n",
      "herseyi soyle hayatında olmaması gerek bence boyle ınsanların falan baskı yapıyosa\n",
      "herşeyi söyle hayatında olmaması gerek bence böyle insanların falan baskı yapıyorsa \n",
      "\n",
      "email adresim zemberek_python@loodos.com\n",
      "mail adresim zemberek_python@loodos.com \n",
      "\n",
      "Kredi başvrusu yapmk istiyrum.\n",
      "kredi başvurusu yapmak istiyorum . \n",
      "\n",
      "Bankanizin hesp blgilerini ogrenmek istyorum.\n",
      "bankanızın hesap bilgilerini öğrenmek istiyorum . \n",
      "\n",
      "2025-01-02 17:51:59,795 - __main__ - INFO\n",
      "Msg: Sentences normalized in: 5.4856274127960205 s\n",
      "\n",
      "2025-01-02 17:52:11,105 - __main__ - INFO\n",
      "Msg: Spell checker instance created in: 11.310301303863525 s\n",
      "\n",
      "okuyablirim = okuyabilirim\n",
      "tartısıyor = tartışıyor tartılıyor\n",
      "Ankar'ada = Ankara'da Ankara'ya Ankara'dan Antara'da Ankaray'a Angara'da Ankaray'da Anakara'da Ankara'nda Ankara'na Ankara'ma Ankara'mda Ankara'ca\n",
      "knlıca = kanlıca kanlıca kınlıca anlıca kılıca kalıca\n",
      "yapablrim = \n",
      "kıredi = kredi küredi\n",
      "geldm = geldi geldim gelim gelem\n",
      "geliyom = geliyor geliyim\n",
      "aldm = aldı alım aldım alem alim alim alam aldo aldi Al'dı Aldo'm Al'ım alda Al'dım Al'da Aldi'm\n",
      "asln = asla aslan aslan aslı aslı asli aslen asan asın Aslı'n aslin As'ın As'lı As'la asin As'lın aslın As'lan assn\n",
      "2025-01-02 17:52:11,858 - __main__ - INFO\n",
      "Msg: Spells checked in: 0.7528433799743652 s\n",
      "\n",
      "Extractor instance created in:  0.031491756439208984  s\n",
      "Sentences separated in 0.0s\n",
      "İnsanoğlu aslında ne para ne sevgi ne kariyer ne şöhret ne de çevre ile sonsuza dek mutlu olabilecek bir yapıya sahiptir.\n",
      "Dış kaynaklardan gelebilecek bu mutluluklar sadece belirli bir zaman için insanı mutlu kılıyor.\n",
      "Kişi bu kaynakları elde ettiği zaman belirli bir dönem için kendini iyi hissediyor, ancak alışma dönemine girdiği andan itibaren bu iyilik hali hızla tükeniyor.\n",
      "Mutlu olma sanatının özü bu değildir.\n",
      "Gerçek mutluluk, kişinin her türlü olaya ve duruma karşı kendini pozitif tutarak mutlu hissedebilmesi halidir.\n",
      "Bu davranış şeklini edinen insan, zor günlerde güçlü, mutlu günlerde zevk alan biri olur ve mutluluğu kalıcı kılar.\n",
      "\n",
      "\n",
      "[Kale:Noun, Prop] kale:Noun+A3sg+m:P1sg+in:Gen\n",
      "[kale:Noun] kale:Noun+A3sg+m:P1sg+in:Gen\n",
      "[kalem:Noun] kalem:Noun+A3sg+in:Gen\n",
      "[kalem:Noun] kalem:Noun+A3sg+in:P2sg\n",
      "\n",
      "\n",
      "\n",
      "Before disambiguation\n",
      "Word = Yarın\n",
      "[yarın:Adv] yarın:Adv\n",
      "[yarmak:Verb] yar:Verb+Imp+ın:A2pl\n",
      "[yar:Noun] yar:Noun+A3sg+ın:Gen\n",
      "[yar:Noun] yar:Noun+A3sg+ın:P2sg\n",
      "[yarı:Noun] yarı:Noun+A3sg+n:P2sg\n",
      "[yarın:Noun, Time] yarın:Noun+A3sg\n",
      "[yarı:Adj] yarı:Adj|Zero→Noun+A3sg+n:P2sg\n",
      "Word = kar\n",
      "[karmak:Verb] kar:Verb+Imp+A2sg\n",
      "[kâr:Noun] kar:Noun+A3sg\n",
      "[kar:Noun] kar:Noun+A3sg\n",
      "Word = yağacak\n",
      "[yağmak:Verb] yağ:Verb+acak:Fut+A3sg\n",
      "[yağmak:Verb] yağ:Verb|acak:FutPart→Adj\n",
      "Word = .\n",
      "[.:Punc] .:Punc\n",
      "\n",
      "After disambiguation\n",
      "[yarın:Noun, Time] yarın:Noun+A3sg\n",
      "[kar:Noun] kar:Noun+A3sg\n",
      "[yağmak:Verb] yağ:Verb+acak:Fut+A3sg\n",
      "[.:Punc] .:Punc\n",
      "Content =  Saat\n",
      "Type =  Word\n",
      "Start =  0\n",
      "Stop =  3 \n",
      "\n",
      "Content =  12:00\n",
      "Type =  Time\n",
      "Start =  5\n",
      "Stop =  9 \n",
      "\n",
      "Content =  .\n",
      "Type =  Punctuation\n",
      "Start =  10\n",
      "Stop =  10 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import logging\n",
    "\n",
    "from zemberek import (\n",
    "    TurkishSpellChecker,\n",
    "    TurkishSentenceNormalizer,\n",
    "    TurkishSentenceExtractor,\n",
    "    TurkishMorphology,\n",
    "    TurkishTokenizer\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "examples = [\"Yrn okua gidicem\",\n",
    "            \"Tmm, yarin havuza giricem ve aksama kadar yaticam :)\",\n",
    "            \"ah aynen ya annemde fark ettı siz evinizden cıkmayın diyo\",\n",
    "            \"gercek mı bu? Yuh! Artık unutulması bile beklenmiyo\",\n",
    "            \"Hayır hayat telaşm olmasa alacam buraları gökdelen dikicem.\",\n",
    "            \"yok hocam kesınlıkle oyle birşey yok\",\n",
    "            \"herseyi soyle hayatında olmaması gerek bence boyle ınsanların falan baskı yapıyosa\",\n",
    "            \"email adresim zemberek_python@loodos.com\",\n",
    "            \"Kredi başvrusu yapmk istiyrum.\",\n",
    "            \"Bankanizin hesp blgilerini ogrenmek istyorum.\"]\n",
    "\n",
    "morphology = TurkishMorphology.create_with_defaults()\n",
    "\n",
    "# SENTENCE NORMALIZATION\n",
    "start = time.time()\n",
    "normalizer = TurkishSentenceNormalizer(morphology)\n",
    "logger.info(f\"Normalization instance created in: {time.time() - start} s\")\n",
    "\n",
    "start = time.time()\n",
    "for example in examples:\n",
    "    print(example)\n",
    "    print(normalizer.normalize(example), \"\\n\")\n",
    "logger.info(f\"Sentences normalized in: {time.time() - start} s\")\n",
    "\n",
    "start = time.time()\n",
    "sc = TurkishSpellChecker(morphology)\n",
    "logger.info(f\"Spell checker instance created in: {time.time() - start} s\")\n",
    "\n",
    "\n",
    "# SPELLING SUGGESTION\n",
    "li = [\"okuyablirim\", \"tartısıyor\", \"Ankar'ada\", \"knlıca\", \"yapablrim\", \"kıredi\", \"geldm\", \"geliyom\", \"aldm\", \"asln\"]\n",
    "start = time.time()\n",
    "for word in li:\n",
    "    print(word + \" = \" + ' '.join(sc.suggest_for_word(word)))\n",
    "logger.info(f\"Spells checked in: {time.time() - start} s\")\n",
    "\n",
    "\n",
    "# SENTENCE BOUNDARY DETECTION\n",
    "start = time.time()\n",
    "extractor = TurkishSentenceExtractor()\n",
    "print(\"Extractor instance created in: \", time.time() - start, \" s\")\n",
    "\n",
    "text = \"İnsanoğlu aslında ne para ne sevgi ne kariyer ne şöhret ne de çevre ile sonsuza dek mutlu olabilecek bir \" \\\n",
    "       \"yapıya sahiptir. Dış kaynaklardan gelebilecek bu mutluluklar sadece belirli bir zaman için insanı mutlu \" \\\n",
    "       \"kılıyor. Kişi bu kaynakları elde ettiği zaman belirli bir dönem için kendini iyi hissediyor, ancak alışma \" \\\n",
    "       \"dönemine girdiği andan itibaren bu iyilik hali hızla tükeniyor. Mutlu olma sanatının özü bu değildir. Gerçek \" \\\n",
    "       \"mutluluk, kişinin her türlü olaya ve duruma karşı kendini pozitif tutarak mutlu hissedebilmesi halidir. Bu \" \\\n",
    "       \"davranış şeklini edinen insan, zor günlerde güçlü, mutlu günlerde zevk alan biri olur ve mutluluğu kalıcı \" \\\n",
    "       \"kılar. \"\n",
    "\n",
    "start = time.time()\n",
    "sentences = extractor.from_paragraph(text)\n",
    "print(f\"Sentences separated in {time.time() - start}s\")\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "print(\"\\n\")\n",
    "\n",
    "# SINGLE WORD MORPHOLOGICAL ANALYSIS\n",
    "results = morphology.analyze(\"kalemin\")\n",
    "for result in results:\n",
    "    print(result)\n",
    "print(\"\\n\")\n",
    "\n",
    "# SENTENCE ANALYSIS AND DISAMBIGUATION\n",
    "\n",
    "sentence = \"Yarın kar yağacak.\"\n",
    "analysis = morphology.analyze_sentence(sentence)\n",
    "after = morphology.disambiguate(sentence, analysis)\n",
    "\n",
    "print(\"\\nBefore disambiguation\")\n",
    "for e in analysis:\n",
    "    print(f\"Word = {e.inp}\")\n",
    "    for s in e:\n",
    "        print(s.format_string())\n",
    "\n",
    "print(\"\\nAfter disambiguation\")\n",
    "for s in after.best_analysis():\n",
    "    print(s.format_string())\n",
    "\n",
    "# TOKENIZATION\n",
    "tokenizer = TurkishTokenizer.DEFAULT\n",
    "\n",
    "tokens = tokenizer.tokenize(\"Saat 12:00.\")\n",
    "for token in tokens:\n",
    "    print('Content = ', token.content)\n",
    "    print('Type = ', token.type_.name)\n",
    "    print('Start = ', token.start)\n",
    "    print('Stop = ', token.end, '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
